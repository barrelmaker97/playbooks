---
- name: Install Helm
  become: true
  community.general.snap:
    name: helm
    classic: true
    channel: latest/edge  # needed because helm 4 breaks things

- name: Install Helm Diff
  kubernetes.core.helm_plugin:
    state: present
    plugin_path: "https://github.com/databus23/helm-diff"

- name: Add Helm Repos
  kubernetes.core.helm_repository:
    name: "{{ item.key }}"
    repo_url: "{{ item.value }}"
  loop: "{{ helm_repos | dict2items }}"
  vars:
    helm_repos:
      grafana: https://grafana.github.io/helm-charts
      grafana-community: https://grafana-community.github.io/helm-charts
      ingress-nginx: https://kubernetes.github.io/ingress-nginx
      jetstack: https://charts.jetstack.io
      longhorn: https://charts.longhorn.io
      metallb: https://metallb.github.io/metallb
      metrics-server: https://kubernetes-sigs.github.io/metrics-server
      open-telemetry: https://open-telemetry.github.io/opentelemetry-helm-charts
      prometheus-community: https://prometheus-community.github.io/helm-charts

- name: Deploy Prometheus CRDs
  kubernetes.core.helm:
    name: prometheus-operator-crds
    chart_ref: prometheus-community/prometheus-operator-crds
    chart_version: "{{ prometheus_crds_version }}"
    release_namespace: monitoring
    update_repo_cache: true
    wait: true

- name: Create Longhorn Namespace
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: longhorn-system
        labels:
          pod-security.kubernetes.io/enforce: privileged
          pod-security.kubernetes.io/audit: privileged
          pod-security.kubernetes.io/warn: privileged

- name: Deploy Longhorn
  kubernetes.core.helm:
    name: longhorn
    chart_ref: longhorn/longhorn
    chart_version: "{{ longhorn_version }}"
    release_namespace: longhorn-system
    create_namespace: true
    update_repo_cache: true
    wait: true
    release_values: "{{ values_file | from_yaml }}"
  vars:
    values_file:
      persistence:
        defaultDataLocality: best-effort
        defaultClassReplicaCount: 2
      metrics:
        serviceMonitor:
          enabled: true
      defaultSettings:
        allowVolumeCreationWithDegradedAvailability: false
        backupTarget: "nfs://{{ nas_domain }}:/volume1/longhorn-backupstore"
        defaultDataLocality: best-effort
        defaultReplicaCount: "{'v1':'2','v2':'2'}"
        replicaAutoBalance: least-effort
        storageMinimalAvailablePercentage: "10"
      ingress:
        enabled: true
        ingressClassName: nginx
        host: "longhorn.{{ cluster_domain }}"

- name: Create Longhorn Local Storage Class
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: longhorn-local
      provisioner: driver.longhorn.io
      allowVolumeExpansion: true
      reclaimPolicy: Retain
      volumeBindingMode: Immediate
      parameters:
        dataEngine: v1
        dataLocality: strict-local
        numberOfReplicas: "1"
        disableRevisionCounter: "true"
        fsType: ext4

- name: Create Default Recurring Snapshot
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: longhorn.io/v1beta2
      kind: RecurringJob
      metadata:
        name: default-snapshotter
        namespace: longhorn-system
      spec:
        concurrency: 1
        cron: 0 * * * ?
        groups:
          - default
        labels: {}
        name: default-snapshotter
        parameters: {}
        retain: 24
        task: snapshot

- name: Create Default Recurring Backup
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: longhorn.io/v1beta2
      kind: RecurringJob
      metadata:
        name: default-backup
        namespace: longhorn-system
      spec:
        concurrency: 1
        cron: 0 12 * * ?
        groups:
          - default
        labels: {}
        name: default-backup
        parameters:
          full-backup-interval: "15"
        retain: 30
        task: backup

- name: Create MetalLB Namespace
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: metallb-system
        labels:
          pod-security.kubernetes.io/enforce: privileged
          pod-security.kubernetes.io/audit: privileged
          pod-security.kubernetes.io/warn: privileged

- name: Deploy MetalLB
  kubernetes.core.helm:
    name: metallb
    chart_ref: metallb/metallb
    chart_version: "{{ metallb_version }}"
    release_namespace: metallb-system
    update_repo_cache: true
    wait: true
    release_values: "{{ values_file | from_yaml }}"
  vars:
    values_file:
      speaker:
        ignoreExcludeLB: true
      prometheus:
        namespace: monitoring
        rbacPrometheus: false
        serviceMonitor:
          enabled: true

- name: Create IP Address Pool
  kubernetes.core.k8s:
    state: present
    template: ipaddresspool-metallb.yaml

- name: Advertise Load Balanced IP
  kubernetes.core.k8s:
    state: present
    definition: "{{ lookup('file', 'files/L2Advertisement-metallb.yaml') | from_yaml }}"

- name: Deploy Ingress Nginx
  kubernetes.core.helm:
    name: ingress-nginx
    chart_ref: ingress-nginx/ingress-nginx
    chart_version: "{{ ingress_nginx_version }}"
    release_namespace: ingress-nginx
    create_namespace: true
    update_repo_cache: true
    wait: true
    release_values: "{{ values_file | from_yaml }}"
  vars:
    values_file:
      controller:
        metrics:
          enabled: true
          serviceMonitor:
            enabled: true

- name: Set Nginx Ingress Controller Custom Headers
  kubernetes.core.k8s:
    state: present
    definition: "{{ lookup('file', 'files/configmap-custom-headers.yaml') | from_yaml }}"

- name: Set Nginx Ingress Controller Config
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: ingress-nginx-controller
        namespace: ingress-nginx
      data:
        add-headers: "ingress-nginx/custom-headers"
        annotation-value-word-blocklist: "load_module,lua_package,_by_lua,location,root,proxy_pass,serviceaccount,{,}"
        enable-brotli: "true"
        hsts-max-age: "63072000"
        hsts-preload: "true"
        ssl-reject-handshake: "true"
        ssl-session-cache: "true"
        ssl-session-cache-size: "10m"
        ssl-session-tickets: "true"

- name: Deploy Cert Manager
  kubernetes.core.helm:
    name: cert-manager
    chart_ref: jetstack/cert-manager
    chart_version: "{{ cert_manager_version }}"
    release_namespace: cert-manager
    create_namespace: true
    update_repo_cache: true
    wait: true
    release_values: "{{ values_file | from_yaml }}"
  vars:
    values_file:
      prometheus:
        enabled: true
        servicemonitor:
          enabled: true
      crds:
        enabled: true

- name: Deploy Trust Manager
  kubernetes.core.helm:
    name: trust-manager
    chart_ref: jetstack/trust-manager
    chart_version: "{{ trust_manager_version }}"
    release_namespace: cert-manager
    update_repo_cache: true
    wait: true
    release_values: "{{ values_file | from_yaml }}"
  vars:
    values_file:
      app:
        trust:
          namespace: cert-manager

- name: Create Internal Self-Signed Issuer
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: cert-manager.io/v1
      kind: ClusterIssuer
      metadata:
        name: selfsigned-issuer
      spec:
        selfSigned: {}

- name: Create Internal Root CA Certificate
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: cert-manager.io/v1
      kind: Certificate
      metadata:
        name: internal-root-ca
        namespace: cert-manager
      spec:
        isCA: true
        commonName: "internal-root-ca"
        duration: 8760h  # 1 year
        renewBefore: 720h  # 30 days
        secretName: internal-root-ca-secret
        privateKey:
          algorithm: ECDSA
          size: 256
        issuerRef:
          name: selfsigned-issuer
          kind: ClusterIssuer
          group: cert-manager.io

- name: Create Internal CA Issuer
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: cert-manager.io/v1
      kind: ClusterIssuer
      metadata:
        name: internal-ca
      spec:
        ca:
          secretName: internal-root-ca-secret

- name: Create Trust Bundle
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: trust.cert-manager.io/v1alpha1
      kind: Bundle
      metadata:
        name: internal-trust
      spec:
        sources:
          - secret:
              name: internal-root-ca-secret
              key: ca.crt
        target:
          configMap:
            key: ca-certificates.crt

- name: Create Production Issuer
  kubernetes.core.k8s:
    state: present
    template: issuer-prod.yaml

- name: Create Staging Issuer
  kubernetes.core.k8s:
    state: present
    template: issuer-staging.yaml

- name: Create Monitoring Namespace
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: monitoring
        labels:
          pod-security.kubernetes.io/enforce: privileged
          pod-security.kubernetes.io/audit: privileged
          pod-security.kubernetes.io/warn: privileged

- name: Create Grafana Admin Credentials Secret
  kubernetes.core.k8s:
    state: present
    namespace: monitoring
    definition: "{{ lookup('community.sops.sops', 'files/secret-grafana-admin-credentials.sops.yaml') | from_yaml }}"

- name: Create Additional Prometheus Scrape Config Secret
  kubernetes.core.k8s:
    state: present
    namespace: monitoring
    definition: "{{ lookup('file', 'files/secret-additional-scrape-config.yaml') | from_yaml }}"

- name: Create Dashboards ConfigMap
  kubernetes.core.k8s:
    state: present
    definition: "{{ lookup('file', 'files/configmap-dashboards.yaml') | from_yaml }}"

- name: Deploy Loki
  kubernetes.core.helm:
    name: loki
    chart_ref: grafana/loki
    chart_version: "{{ loki_version }}"
    release_namespace: monitoring
    update_repo_cache: true
    wait: true
    release_values: "{{ values_file | from_yaml }}"
  vars:
    values_file:
      loki:
        auth_enabled: false
        commonConfig:
          replication_factor: 1
        storage:
          type: filesystem
        schemaConfig:
          configs:
            - from: "2024-04-01"
              store: tsdb
              object_store: filesystem
              schema: v13
              index:
                prefix: index_
                period: 24h
        compactor:
          retention_enabled: true
          delete_request_store: filesystem
        limits_config:
          retention_period: 720h  # 30 days
          max_query_lookback: 720h  # 30 days
      deploymentMode: SingleBinary
      singleBinary:
        replicas: 1
        persistence:
          enabled: true
          storageClass: longhorn
          size: 10Gi
      # These flags are required to disable the default S3 requirements in 6.x
      backend:
        replicas: 0
      read:
        replicas: 0
      write:
        replicas: 0
      resultsCache:
        enabled: false
      chunksCache:
        enabled: false

- name: Deploy Tempo
  kubernetes.core.helm:
    name: tempo
    chart_ref: grafana-community/tempo
    chart_version: "{{ tempo_version }}"
    release_namespace: monitoring
    update_repo_cache: true
    wait: true
    release_values: "{{ values_file | from_yaml }}"
  vars:
    values_file:
      tempo:
        retention: 720h
      persistence:
        enabled: true
        storageClass: longhorn
        size: 10Gi

- name: Deploy OTel Collector
  kubernetes.core.helm:
    name: otel-collector
    chart_ref: open-telemetry/opentelemetry-collector
    chart_version: "{{ otel_collector_version }}"
    release_namespace: monitoring
    update_repo_cache: true
    wait: true
    release_values: "{{ values_file | from_yaml }}"
  vars:
    values_file:
      image:
        repository: "otel/opentelemetry-collector-contrib"
      command:
        name: "otelcol-contrib"
      mode: deployment
      config:
        receivers:
          otlp:
            protocols:
              grpc:
                endpoint: 0.0.0.0:4317
              http:
                endpoint: 0.0.0.0:4318
        exporters:
          otlp_grpc/tempo:
            endpoint: tempo.monitoring.svc.cluster.local:4317
            tls:
              insecure: true
          otlp_http/loki:
            endpoint: http://loki.monitoring.svc.cluster.local:3100/otlp/v1/logs
          prometheusremotewrite:
            endpoint: http://prometheus-operated.monitoring.svc.cluster.local:9090/api/v1/write
            tls:
              insecure: true
        service:
          pipelines:
            traces:
              receivers: [otlp]
              exporters: [otlp_grpc/tempo]
            metrics:
              receivers: [otlp]
              exporters: [prometheusremotewrite]
            logs:
              receivers: [otlp]
              exporters: [otlp_http/loki]

- name: Deploy Prometheus Stack
  kubernetes.core.helm:
    name: kube-prometheus-stack
    chart_ref: prometheus-community/kube-prometheus-stack
    chart_version: "{{ kube_prometheus_stack_version }}"
    release_namespace: monitoring
    update_repo_cache: true
    wait: true
    release_values: "{{ values_file | from_yaml }}"
  vars:
    values_file:
      kubeScheduler:
        service:
          selector:
            k8s-app: kube-scheduler
      kubeControllerManager:
        service:
          selector:
            k8s-app: kube-controller-manager
      alertmanager:
        alertmanagerSpec:
          alertmanagerConfigMatcherStrategy:
            type: None
          storage:
            volumeClaimTemplate:
              spec:
                storageClassName: longhorn
                accessModes:
                  - ReadWriteOnce
                resources:
                  requests:
                    storage: 10Gi
        ingress:
          enabled: true
          ingressClassName: nginx
          hosts:
            - "alertmanager.{{ cluster_domain }}"
      grafana:
        admin:
          existingSecret: grafana-admin-credentials
        additionalDataSources:
          - name: Loki
            type: loki
            url: http://loki.monitoring.svc.cluster.local:3100
            access: proxy
            isDefault: false
          - name: Tempo
            type: tempo
            url: http://tempo.monitoring.svc.cluster.local:3200
            access: proxy
            isDefault: false
            jsonData:
              lokiSearch:
                datasourceUid: 'Loki'
        grafana.ini:
          auth:
            login_cookie_name: __Host-grafana_session
          security:
            cookie_samesite: strict
            cookie_secure: true
            x_content_type_options: false
            content_security_policy: true
            content_security_policy_template: >-
              """
              base-uri 'self';
              connect-src 'self' grafana.com ws://$ROOT_PATH wss://$ROOT_PATH data:;
              default-src 'none';
              font-src 'self';
              form-action 'self';
              frame-ancestors 'self';
              img-src 'self' https://grafana.com data:;
              manifest-src 'self';
              media-src 'none';
              object-src 'none';
              script-src 'unsafe-eval' 'strict-dynamic' $NONCE;
              style-src 'self' 'unsafe-inline' blob:;
              worker-src 'self' blob:;
              """
        ingress:
          enabled: true
          ingressClassName: nginx
          annotations:
            cert-manager.io/cluster-issuer: letsencrypt-prod
          hosts:
            - "grafana.{{ external_domain }}"
          tls:
            - secretName: grafana-tls
              hosts:
                - "grafana.{{ external_domain }}"
      prometheus:
        ingress:
          enabled: true
          ingressClassName: nginx
          hosts:
            - "prometheus.{{ cluster_domain }}"
        prometheusSpec:
          retention: 30d
          ruleSelectorNilUsesHelmValues: false
          serviceMonitorSelectorNilUsesHelmValues: false
          additionalScrapeConfigsSecret:
            enabled: true
            name: additional-scrape-configs
            key: prometheus-additional.yaml
          storageSpec:
            volumeClaimTemplate:
              spec:
                storageClassName: longhorn
                accessModes:
                  - ReadWriteOnce
                resources:
                  requests:
                    storage: 20Gi

- name: Create Alert Manager Config
  kubernetes.core.k8s:
    state: present
    namespace: monitoring
    definition: "{{ lookup('file', 'files/alertmanagerconfig.yaml') | from_yaml }}"

- name: Create Discord Webhook URL Secret
  kubernetes.core.k8s:
    state: present
    namespace: monitoring
    definition: "{{ lookup('community.sops.sops', 'files/secret-discord-webhook-url.sops.yaml') | from_yaml }}"

- name: Deploy Metrics Server
  kubernetes.core.helm:
    name: metrics-server
    chart_ref: metrics-server/metrics-server
    chart_version: "{{ metrics_server_version }}"
    release_namespace: kube-system
    update_repo_cache: true
    wait: true
    release_values: "{{ values_file | from_yaml }}"
  vars:
    values_file:
      args:
        - --kubelet-insecure-tls
      metrics:
        enabled: true
      serviceMonitor:
        enabled: true
